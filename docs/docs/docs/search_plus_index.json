{"./":{"url":"./","title":"Introduction","keywords":"","body":"Istio 运维实战 前言 通过将微服务中原本在 SDK 中实现的应用流量管理、可见性、通信安全等服务治理能力下放到一个专门的“服务网格”基础设施中，Istio 解开了微服务的服务治理需求和业务逻辑之间的代码、编译、部署时机等的耦合，让微服务真正做到了承诺的“按需选择开发语言”，“独立部署升级”等能力，提升了微服务开发和部署的敏捷性，释放了微服务模式的生产力。 然而，“服务网格”这一基础设施的引入也给整个微服务的运维技术栈带来了新的挑战。对于运维同学来说，Istio 和 Envoy 的运维存在着较陡的学习曲线。TCM（Tencent Cloud Mesh）团队是业内最早一批接触服务网格技术的人员之一，有着大量 Istio/Envoy 故障排查和运维经验。本电子书记录了 TCM 团队从大量实际案例中总结出来的 Istio 运维经验，以及使用 Istio 的最佳实践，希望对大家有所帮助。 Copyright © Aeraki Framework 2021 all right reserved，powered by Gitbook Updated at 2021-09-26 14:56:48 "},"content/debug-istio/":{"url":"content/debug-istio/","title":"Istio 调试指南","keywords":"","body":"服务网格为微服务提供了一个服务通信的基础设施层，统一为上层的微服务提供了服务发现，负载均衡，重试，断路等基础通信功能，以及服务路由，灰度发布，chaos测试等高级管控功能。 服务网格的引入大大降低了个微服务应用的开发难度，让微服务应用开发人员不再需要花费大量时间用于保障底层通讯的正确性上，而是重点关注于产生用户价值的业务需求。 然而由于微服务架构的分布式架构带来的复杂度并未从系统中消失，而是从各个微服务应用中转移到了服务网格中。由服务网格对所有微服务应用的通讯进行统一控制，好处是可以保证整个系统中分布式通讯策略的一致性，并可以方便地进行集中管控。 除微服务之间分布式调用的复杂度之外，服务网格在底层通讯和微服务应用之间引入了新的抽象层，为系统引入了一些额外的复杂度。在此情况下，如果服务网格自身出现故障，将对上层的微服务应用带来灾难性的影响。 当系统中各微服务应用之间的通讯出现异常时，我们可以通过服务网格提供的分布式调用跟踪，故障注入，服务路由等手段快速进行分析和处理。但如果服务网格系统自身出现问题的话，我们如何才能快速进行分析处理呢？ Copyright © Aeraki Framework 2021 all right reserved，powered by Gitbook Updated at 2021-09-24 19:52:40 "},"content/common-problem/":{"url":"content/common-problem/","title":"Istio 常见问题","keywords":"","body":"介绍在使用 Istio 过程中可能遇到的一些常见问题的解决方法. Copyright © Aeraki Framework 2021 all right reserved，powered by Gitbook Updated at 2021-09-26 15:43:36 "},"content/best-practice/":{"url":"content/best-practice/","title":"Istio 最佳实践","keywords":"","body":"介绍用户从 Spring Cloud，Dubbo 等传统微服务框架迁移到 Istio 服务网格时的最佳实践。 Copyright © Aeraki Framework 2021 all right reserved，powered by Gitbook Updated at 2021-09-26 15:41:27 "},"content/best-practice/method-level-tracing.html":{"url":"content/best-practice/method-level-tracing.html","title":"在 Istio 中实现方法级调用跟踪","keywords":"","body":"本文将通过一个网上商店的示例程序介绍如何利用 Spring 和 OpenTracing 简化应用程序的 Tracing 上下文传递，以及如何在 Istio 提供的进程间调用跟踪基础上实现方法级别的细粒度调用跟踪。 分布式调用跟踪和OpenTracing规范 什么是分布式调用跟踪？ 相比传统的“巨石”应用，微服务的一个主要变化是将应用中的不同模块拆分为了独立的进程。在微服务架构下，原来进程内的方法调用成为了跨进程的RPC调用。相对于单一进程的方法调用，跨进程调用的调试和故障分析是非常困难的，很难用传统的调试器或者日志打印来对分布式调用进行查看和分析。 如上图所示，一个来自客户端的请求经过了多个微服务进程。如果要对该请求进行分析，则必须将该请求经过的所有服务的相关信息都收集起来并关联在一起，这就是“分布式调用跟踪”。 什么是OpenTracing？ CNCF OpenTracing项目 OpenTracing是CNCF（云原生计算基金会）下的一个项目，其中包含了一套分布式调用跟踪的标准规范，各种语言的API，编程框架和函数库。OpenTracing的目的是定义一套分布式调用跟踪的标准，以统一各种分布式调用跟踪的实现。目前已有大量支持OpenTracing规范的Tracer实现，包括Jager,Skywalking,LightStep等。在微服务应用中采用OpenTracing API实现分布式调用跟踪，可以避免vendor locking，以最小的代价和任意一个兼容OpenTracing的基础设施进行对接。 OpenTracing概念模型 OpenTracing的概念模型参见下图： 图源自 https://opentracing.io/ 如图所示，OpenTracing中主要包含下述几个概念： Trace： 描述一个分布式系统中的端到端事务，例如来自客户端的一个请求。 Span：一个具有名称和时间长度的操作，例如一个REST调用或者数据库操作等。Span是分布式调用跟踪的最小跟踪单位，一个Trace由多段Span组成。 Span context：分布式调用跟踪的上下文信息，包括Trace id，Span id以及其它需要传递到下游服务的内容。一个OpenTracing的实现需要将Span context通过某种序列化机制(Wire Protocol)在进程边界上进行传递，以将不同进程中的Span关联到同一个Trace上。这些Wire Protocol可以是基于文本的，例如HTTP header，也可以是二进制协议。 OpenTracing数据模型 一个Trace可以看成由多个相互关联的Span组成的有向无环图（DAG图）。下图是一个由8个Span组成的Trace： [Span A] ←←←(the root span) | +------+------+ | | [Span B] [Span C] ←←←(Span C is a `ChildOf` Span A) | | [Span D] +---+-------+ | | [Span E] [Span F] >>> [Span G] >>> [Span H] ↑ ↑ ↑ (Span G `FollowsFrom` Span F) 上图的trace也可以按照时间先后顺序表示如下： ––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–––––––|–> time [Span A···················································] [Span B··············································] [Span D··········································] [Span C········································] [Span E·······] [Span F··] [Span G··] [Span H··] Span的数据结构中包含以下内容： name: Span所代表的操作名称，例如REST接口对应的资源名称。 Start timestamp: Span所代表操作的开始时间 Finish timestamp: Span所代表的操作的的结束时间 Tags：一系列标签，每个标签由一个key value键值对组成。该标签可以是任何有利于调用分析的信息，例如方法名，URL等。 SpanContext：用于跨进程边界传递Span相关信息，在进行传递时需要结合一种序列化协议（Wire Protocol）使用。 References：该Span引用的其它关联Span，主要有两种引用关系，Childof和FollowsFrom。 Childof： 最常用的一种引用关系，表示Parent Span和Child Span之间存在直接的依赖关系。例RPC服务端Span和RPC客户端Span，或者数据库SQL插入Span和ORM Save动作Span之间的关系。 FollowsFrom：如果Parent Span并不依赖Child Span的执行结果，则可以用FollowsFrom表示。例如网上商店购物付款后会向用户发一个邮件通知，但无论邮件通知是否发送成功，都不影响付款成功的状态，这种情况则适用于用FollowsFrom表示。 跨进程调用信息传播 SpanContext是OpenTracing中一个让人比较迷惑的概念。在OpenTracing的概念模型中提到SpanContext用于跨进程边界传递分布式调用的上下文。但实际上OpenTracing只定义一个SpanContext的抽象接口，该接口封装了分布式调用中一个Span的相关上下文内容，包括该Span所属的Trace id，Span id以及其它需要传递到downstream服务的信息。SpanContext自身并不能实现跨进程的上下文传递，需要由Tracer（Tracer是一个遵循OpenTracing协议的实现，如Jaeger，Skywalking的Tracer）将SpanContext序列化后通过Wire Protocol传递到下一个进程中，然后在下一个进程将SpanContext反序列化，得到相关的上下文信息，以用于生成Child Span。 为了为各种具体实现提供最大的灵活性，OpenTracing只是提出了跨进程传递SpanContext的要求，并未规定将SpanContext进行序列化并在网络中传递的具体实现方式。各个不同的Tracer可以根据自己的情况使用不同的Wire Protocol来传递SpanContext。 在基于HTTP协议的分布式调用中，通常会使用HTTP Header来传递SpanContext的内容。常见的Wire Protocol包含Zipkin使用的b3 HTTP header，Jaeger使用的uber-trace-id HTTP Header,LightStep使用的\"x-ot-span-context\" HTTP Header等。Istio/Envoy支持b3 header和x-ot-span-context header,可以和Zipkin,Jaeger及LightStep对接。其中b3 HTTP header的示例如下： X-B3-TraceId: 80f198ee56343ba864fe8b2a57d3eff7 X-B3-ParentSpanId: 05e3ac9a4f6e3b90 X-B3-SpanId: e457b5a2e4d86bd1 X-B3-Sampled: 1 Istio对分布式调用跟踪的支持 Istio/Envoy为微服务提供了开箱即用的分布式调用跟踪功能。在安装了Istio和Envoy的微服务系统中，Envoy会拦截服务的入向和出向请求，为微服务的每个调用请求自动生成调用跟踪数据。通过在服务网格中接入一个分布式跟踪的后端系统，例如zipkin或者Jaeger，就可以查看一个分布式请求的详细内容，例如该请求经过了哪些服务，调用了哪个REST接口，每个REST接口所花费的时间等。 需要注意的是，Istio/Envoy虽然在此过程中完成了大部分工作，但还是要求对应用代码进行少量修改：应用代码中需要将收到的上游HTTP请求中的b3 header拷贝到其向下游发起的HTTP请求的header中，以将调用跟踪上下文传递到下游服务。这部分代码不能由Envoy代劳，原因是Envoy并不清楚其代理的服务中的业务逻辑，无法将入向请求和出向请求按照业务逻辑进行关联。这部分代码量虽然不大，但需要对每一处发起HTTP请求的代码都进行修改，非常繁琐而且容易遗漏。当然，可以将发起HTTP请求的代码封装为一个代码库来供业务模块使用，来简化该工作。 下面以一个简单的网上商店示例程序来展示Istio如何提供分布式调用跟踪。该示例程序由eshop,inventory，billing，delivery几个微服务组成，结构如下图所示： eshop微服务接收来自客户端的请求，然后调用inventory，billing，delivery这几个后端微服务的REST接口来实现用户购买商品的checkout业务逻辑。本例的代码可以从github下载：https://github.com/aeraki-framework/method-level-tracing-with-istio 如下面的代码所示，我们需要在eshop微服务的应用代码中传递b3 HTTP Header。 @RequestMapping(value = \"/checkout\") public String checkout(@RequestHeader HttpHeaders headers) { String result = \"\"; // Use HTTP GET in this demo. In a real world use case,We should use HTTP POST // instead. // The three services are bundled in one jar for simplicity. To make it work, // define three services in Kubernets. result += restTemplate.exchange(\"http://inventory:8080/createOrder\", HttpMethod.GET, new HttpEntity<>(passTracingHeader(headers)), String.class).getBody(); result += \"\"; result += restTemplate.exchange(\"http://billing:8080/payment\", HttpMethod.GET, new HttpEntity<>(passTracingHeader(headers)), String.class).getBody(); result += \"\"; result += restTemplate.exchange(\"http://delivery:8080/arrangeDelivery\", HttpMethod.GET, new HttpEntity<>(passTracingHeader(headers)), String.class).getBody(); return result; } private HttpHeaders passTracingHeader(HttpHeaders headers) { HttpHeaders tracingHeaders = new HttpHeaders(); extractHeader(headers, tracingHeaders, \"x-request-id\"); extractHeader(headers, tracingHeaders, \"x-b3-traceid\"); extractHeader(headers, tracingHeaders, \"x-b3-spanid\"); extractHeader(headers, tracingHeaders, \"x-b3-parentspanid\"); extractHeader(headers, tracingHeaders, \"x-b3-sampled\"); extractHeader(headers, tracingHeaders, \"x-b3-flags\"); extractHeader(headers, tracingHeaders, \"x-ot-span-context\"); return tracingHeaders; } 下面我们来测试一下eshop实例程序。我们可以自己搭建一个Kubernetes集群并安装Istio以用于测试。这里为了方便，直接使用腾讯云上提供的全托管的服务网格 TCM，并在创建的 Mesh 中加入了一个容器服务TKE 集群来进行测试。 在 TKE 集群中部署该程序，查看Istio分布式调用跟踪的效果。 git clone git@github.com:aeraki-framework/method-level-tracing-with-istio.git cd method-level-tracing-with-istio git checkout without-opentracing kubectl apply -f k8s/eshop.yaml 在浏览器中打开地址：http://${INGRESS_EXTERNAL_IP}/checkout ，以触发调用eshop示例程序的REST接口。 在浏览器中打开 TCM 的界面，查看生成的分布式调用跟踪信息。 TCM 图形界面直观地展示了这次调用的详细信息，可以看到客户端请求从Ingressgateway进入到系统中，然后调用了eshop微服务的checkout接口，checkout调用有三个child span，分别对应到inventory，billing和delivery三个微服务的REST接口。 使用OpenTracing来传递分布式跟踪上下文 OpenTracing提供了基于Spring的代码埋点，因此我们可以使用OpenTracing Spring框架来提供HTTP header的传递，以避免这部分硬编码工作。在Spring中采用OpenTracing来传递分布式跟踪上下文非常简单，只需要下述两个步骤： 在Maven POM文件中声明相关的依赖，一是对OpenTracing SPring Cloud Starter的依赖；另外由于Istio 采用了Zipkin的上报接口，我们也需要引入Zipkin的相关依赖。 在Spring Application中声明一个Tracer bean。如下所示，注意我们需要把 Istio 中的zpkin上报地址设置到OKHttpSernder中。 @Bean public io.opentracing.Tracer zipkinTracer() { String zipkinEndpoint = System.getenv(\"ZIPKIN_ENDPOINT\"); if (zipkinEndpoint == null || zipkinEndpoint == \"\"){ zipkinEndpoint = \"http://zipkin.istio-system:9411/api/v2/spans\"; } OkHttpSender sender = OkHttpSender.create(zipkinEndpoint); Reporter spanReporter = AsyncReporter.create(sender); Tracing braveTracing = Tracing.newBuilder() .localServiceName(\"my-service\") .propagationFactory(B3Propagation.FACTORY) .spanReporter(spanReporter) .build(); Tracing braveTracer = Tracing.newBuilder() .localServiceName(\"spring-boot\") .spanReporter(spanReporter) .propagationFactory(B3Propagation.FACTORY) .traceId128Bit(true) .sampler(Sampler.ALWAYS_SAMPLE) .build(); return BraveTracer.create(braveTracer); } 部署采用OpenTracing进行HTTP header传递的程序版本，其调用跟踪信息如下所示： 从上图中可以看到，相比在应用代码中直接传递HTTP header的方式，采用OpenTracing进行代码埋点后，相同的调用增加了7个名称前缀为spring-boot的Span，这7个Span是由OpenTracing的tracer生成的。虽然我们并没有在代码中显示创建这些Span，但OpenTracing的代码埋点会自动为每一个REST请求生成一个Span，并根据调用关系关联起来。 OpenTracing生成的这些Span为我们提供了更详细的分布式调用跟踪信息，从这些信息中可以分析出一个HTTP调用从客户端应用代码发起请求，到经过客户端的Envoy，再到服务端的Envoy，最后到服务端接受到请求各个步骤的耗时情况。从图中可以看到，Envoy转发的耗时在1毫秒左右，相对于业务代码的处理时长非常短，对这个应用而言，Envoy的处理和转发对于业务请求的处理效率基本没有影响。 在Istio调用跟踪链中加入方法级的调用跟踪信息 Istio/Envoy提供了跨服务边界的调用链信息，在大部分情况下，服务粒度的调用链信息对于系统性能和故障分析已经足够。但对于某些服务，需要采用更细粒度的调用信息来进行分析，例如一个REST请求内部的业务逻辑和数据库访问分别的耗时情况。在这种情况下，我们需要在服务代码中进行埋点，并将服务代码中上报的调用跟踪数据和Envoy生成的调用跟踪数据进行关联，以统一呈现Envoy和服务代码中生成的调用数据。 在方法中增加调用跟踪的代码是类似的，因此我们用AOP + Annotation的方式实现，以简化代码。 首先定义一个Traced注解和对应的AOP实现逻辑： @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) @Documented public @interface Traced { } @Aspect @Component public class TracingAspect { @Autowired Tracer tracer; @Around(\"@annotation(com.zhaohuabing.demo.instrument.Traced)\") public Object aroundAdvice(ProceedingJoinPoint jp) throws Throwable { String class_name = jp.getTarget().getClass().getName(); String method_name = jp.getSignature().getName(); Span span = tracer.buildSpan(class_name + \".\" + method_name).withTag(\"class\", class_name) .withTag(\"method\", method_name).start(); Object result = jp.proceed(); span.finish(); return result; } } 然后在需要进行调用跟踪的方法上加上Traced注解： @Component public class DBAccess { @Traced public void save2db() { try { Thread.sleep((long) (Math.random() * 100)); } catch (InterruptedException e) { e.printStackTrace(); } } } @Component public class BankTransaction { @Traced public void transfer() { try { Thread.sleep((long) (Math.random() * 100)); } catch (InterruptedException e) { e.printStackTrace(); } } } demo程序的master branch已经加入了方法级代码跟踪，可以直接部署。 git checkout master kubectl apply -f k8s/eshop.yaml 效果如下图所示，可以看到trace中增加了transfer和save2db两个方法级的Span。 可以打开一个方法的Span，查看详细信息，包括Java类名和调用的方法名等，在AOP代码中还可以根据需要添加出现异常时的异常堆栈等信息。 总结 Istio/Envoy为微服务应用提供了分布式调用跟踪功能，提高了服务调用的可见性。我们可以使用OpenTracing来代替应用硬编码，以传递分布式跟踪的相关http header；还可以通过OpenTracing将方法级的调用信息加入到Istio/Envoy缺省提供的调用链跟踪信息中，以提供更细粒度的调用跟踪信息。 下一步 除了同步调用之外，异步消息也是微服务架构中常见的一种通信方式。在下一篇文章中，我将继续利用eshop demo程序来探讨如何通过OpenTracing将Kafka异步消息也纳入到Istio的分布式调用跟踪中。 参考资料 本文中eshop示例程序的源代码 Opentracing docs Opentracing specification Opentracing wire protocols Istio Trace context propagation Zipkin-b3-propagation OpenTracing Project Deep Dive Copyright © Aeraki Framework 2021 all right reserved，powered by Gitbook Updated at 2021-09-26 15:29:40 "},"content/best-practice/async-message-tracing.html":{"url":"content/best-practice/async-message-tracing.html","title":"在 Istio 中实现异步消息调用跟踪","keywords":"","body":"在实际项目中，除了同步调用之外，异步消息也是微服务架构中常见的一种通信方式。在本篇文章中，我将继续利用eshop demo程序来探讨如何通过OpenTracing将Kafka异步消息也纳入到Istio的分布式调用跟踪中。 eshop 示例程序结构 如下图所示，demo程序中增加了发送和接收Kafka消息的代码。eshop微服务在调用inventory，billing，delivery服务后，发送了一个kafka消息通知，consumer接收到通知后调用notification服务的REST接口向用户发送购买成功的邮件通知。 将Kafka消息处理加入调用链跟踪 植入Kafka OpenTracing代码 首先从github下载代码。 git clone git@github.com:aeraki-framework/method-level-tracing-with-istio.git 可以直接使用该代码，但建议跟随下面的步骤查看相关的代码，以了解各个步骤背后的原理。 根目录下分为了rest-service和kafka-consumer两个目录，rest-service下包含了各个REST服务的代码，kafka-consumer下是Kafka消息消费者的代码。 首先需要将spring kafka和OpenTracing kafka的依赖加入到两个目录下的pom文件中。 org.springframework.kafka spring-kafka io.opentracing.contrib opentracing-kafka-client ${version.opentracing.kafka-client} 在rest-service目录中的KafkaConfig.java中配置消息Producer端的OpenTracing Instrument。TracingProducerInterceptor会在发送Kafka消息时生成发送端的Span。 @Bean public ProducerFactory producerFactory() { Map configProps = new HashMap<>(); configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress); configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); configProps.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, TracingProducerInterceptor.class.getName()); return new DefaultKafkaProducerFactory<>(configProps); } 在kafka-consumer目录中的KafkaConfig.java中配置消息Consumer端的OpenTracing Instrument。TracingConsumerInterceptor会在接收到Kafka消息是生成接收端的Span。 @Bean public ConsumerFactory consumerFactory() { Map props = new HashMap<>(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress); props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, TracingConsumerInterceptor.class.getName()); return new DefaultKafkaConsumerFactory<>(props); } 只需要这两步即可完成Spring程序的Kafka OpenTracing代码植入。下面安装并运行示例程序查看效果。 安装Kafka集群 示例程序中使用到了Kafka消息，因此我们在 TKE 集群中部署一个简单的Kafka实例： cd method-level-tracing-with-istio kubectl apply -f k8s/kafka.yaml 部署demo应用 修改Kubernetes yaml部署文件 k8s/eshop.yaml，设置Kafka bootstrap server，以用于demo程序连接到Kafka集群中。 apiVersion: apps/v1 kind: Deployment metadata: name: delivery ...... spec: containers: - name: eshop image: aeraki/istio-opentracing-demo:latest ports: - containerPort: 8080 env: .... //在这里加入Kafka server地址 - name: KAFKA_BOOTSTRAP_SERVERS value: \"kafka-service:9092\" --- apiVersion: apps/v1 kind: Deployment metadata: name: kafka-consumer ...... spec: containers: - name: kafka-consumer image: aeraki/istio-opentracing-demo-kafka-consumer:latest env: .... //在这里加入Kafka server地址 - name: KAFKA_BOOTSTRAP_SERVERS value: \"kafka-service:9092\" 然后部署应用程序，相关的镜像可以直接从dockerhub下载，也可以通过源码编译生成。 kubectl apply -f k8s/eshop.yaml 在浏览器中打开地址：http://${INGRESS_EXTERNAL_IP}/checkout ，以触发调用eshop示例程序的REST接口。然后打开 TCM 的界面查看生成的分布式调用跟踪信息。 从图中可以看到，在调用链中增加了两个Span，分布对应于Kafka消息发送和接收的两个操作。由于Kafka消息的处理是异步的，消息发送端不直接依赖接收端的处理。根据OpenTracing对引用关系的定义，From_eshop_topic Span 对 To_eshop_topic Span 的引用关系是 FOLLOWS_FROM 而不是 CHILD_OF 关系。 将调用跟踪上下文从Kafka传递到REST服务 现在eshop代码中已经加入了REST和Kafka的OpenTracing Instrumentation，可以在进行REST调用和发送Kafka消息时生成调用跟踪信息。但如果需要从Kafka的消息消费者的处理方法中调用一个REST接口呢？ 我们会发现在eshop示例程序中，缺省生成的调用链里面并不会把Kafka消费者的Span和其发起的调用notification服务的REST请求的Span关联在同一个Trace中。 要分析导致该问题的原因，我们首先需要了解“Active Span”的概念。在OpenTracing中，一个线程可以有一个Active Span，该Active Span代表了目前该线程正在执行的工作。在调用Tracer.buildSpan()方法创建新的Span时，如果Tracer目前存在一个Active Span，则会将该Active Span缺省作为新创建的Span的Parent Span。 Tracer.buildSpan 方法的说明如下： Tracer.SpanBuilder buildSpan(String operationName) Return a new SpanBuilder for a Span with the given `operationName`. You can override the operationName later via BaseSpan.setOperationName(String). A contrived example: Tracer tracer = ... // Note: if there is a `tracer.activeSpan()`, it will be used as the target of an implicit CHILD_OF // Reference for \"workSpan\" when `startActive()` is invoked. // 如果存在active span，则其创建的新Span会隐式地创建一个 CHILD_OF 引用到该active span try (ActiveSpan workSpan = tracer.buildSpan(\"DoWork\").startActive()) { workSpan.setTag(\"...\", \"...\"); // etc, etc } // 也可以通过asChildOf方法指定新创建的Span的Parent Span // It's also possible to create Spans manually, bypassing the ActiveSpanSource activation. Span http = tracer.buildSpan(\"HandleHTTPRequest\") .asChildOf(rpcSpanContext) // an explicit parent .withTag(\"user_agent\", req.UserAgent) .withTag(\"lucky_number\", 42) .startManual(); 分析Kafka OpenTracing Instrumentation的代码，会发现TracingConsumerInterceptor在调用Kafka消费者的处理方法之前已经把消费者的Span结束了，因此发起REST调用时tracer没有active span，不会将Kafka消费者的Span作为后面REST调用的parent span。 public static void buildAndFinishChildSpan(ConsumerRecord record, Tracer tracer, BiFunction consumerSpanNameProvider) { SpanContext parentContext = TracingKafkaUtils.extractSpanContext(record.headers(), tracer); String consumerOper = FROM_PREFIX + record.topic(); // 此时TracingConsumerInterceptor已经将Kafka消费者的Span放到了Kafka消息的header中，因此从Kafka消息头中取出该Span，显示地将Kafka消费者的Span作为REST调用的Parent Span即可。 为MessageConsumer.java使用的RestTemplate设置一个TracingKafka2RestTemplateInterceptor。 @KafkaListener(topics = \"eshop-topic\") public void receiveMessage(ConsumerRecord record) { restTemplate .setInterceptors(Collections.singletonList(new TracingKafka2RestTemplateInterceptor(record.headers()))); restTemplate.getForEntity(\"http://notification:8080/sendEmail\", String.class); } TracingKafka2RestTemplateInterceptor是基于Spring OpenTracing Instrumentation的TracingRestTemplateInterceptor修改的，将从Kafka header中取出的Span设置为出向请求的Span的Parent Span。 @Override public ClientHttpResponse intercept(HttpRequest httpRequest, byte[] body, ClientHttpRequestExecution xecution) throws IOException { ClientHttpResponse httpResponse; SpanContext parentSpanContext = TracingKafkaUtils.extractSpanContext(headers, tracer); Span span = tracer.buildSpan(httpRequest.getMethod().toString()).asChildOf(parentSpanContext) .withTag(Tags.SPAN_KIND.getKey(), Tags.SPAN_KIND_CLIENT).start(); ...... } 在浏览器中打开地址：http://${INGRESS_EXTERNAL_IP}/checkout ，以触发调用eshop示例程序的REST接口。然后打开 TCM 的界面查看生成的分布式调用跟踪信息。 从上图可以看到，调用链中出现了Kafka消费者调用notification服务的sendEmail REST接口的Span。从图中可以看到，由于调用链经过了Kafka消息，sendEmail Span的时间没有包含在checkout Span中。 总结 Istio服务网格通过分布式调用跟踪来提高微服务应用的可见性，这需要在应用程序中通过HTTP header传递调用跟踪的上下文。对于JAVA应用程序，我们可以使用OpenTracing Instrumentation来代替应用编码传递分布式跟踪的相关http header，以减少对业务代码的影响；我们还可以将方法级的调用跟踪和Kafka消息的调用跟踪加入到Istio生成的调用跟踪链中，以为应用程序的故障定位提供更为丰富详细的调用跟踪信息。 参考资料 本文中eshop示例程序的源代码 Copyright © Aeraki Framework 2021 all right reserved，powered by Gitbook Updated at 2021-09-26 15:35:16 "},"content/tcm/":{"url":"content/tcm/","title":"腾讯云服务网格 TCM","keywords":"","body":"TCM（Tencent Cloud Mesh）是腾讯云上提供的基于Istio进行增强，和Istio API完全兼容的Service Mesh托管服务，可以帮助用户以较小的迁移成本和维护代价快速利用到Service Mesh提供的流量管理和服务治理能力。 Copyright © Aeraki Framework 2021 all right reserved，powered by Gitbook Updated at 2021-09-26 15:40:03 "}}